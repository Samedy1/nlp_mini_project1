{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import *\n",
    "\n",
    "with open('corpus/test.txt', 'r') as infile:\n",
    "    file_content = infile.read().replace('\\n', '')\n",
    "\n",
    "text_preprocessor = TextPreprocessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpolationAddK:\n",
    "    def __init__(\n",
    "        self,\n",
    "        k=0.1,  # Smoothing parameter\n",
    "        lambda1=0.1,\n",
    "        lambda2=0.2,\n",
    "        lambda3=0.3,\n",
    "        lambda4=0.4,\n",
    "    ) -> None:\n",
    "        # preprocess\n",
    "        text_preprocessor = TextPreprocessor()\n",
    "        \n",
    "        # initialize necessary fields\n",
    "        self.freq_uni = text_preprocessor.freq_uni\n",
    "        self.freq_bi = text_preprocessor.freq_bi\n",
    "        self.freq_tri = text_preprocessor.freq_tri\n",
    "        self.freq_four = text_preprocessor.freq_four\n",
    "        \n",
    "        # k and lambda\n",
    "        self.k = k\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        self.lambda3 = lambda3\n",
    "        self.lambda4 = lambda4\n",
    "    \n",
    "    # ---------- interpolation with Add-k probability of unigram ----------\n",
    "    def probability(\n",
    "        self,\n",
    "        word: str,\n",
    "        given_tri_gram: tuple,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Estimate the probability of a word being the next word after given previous words using linear interpolation with add-k smoothing.\n",
    "\n",
    "        Args:\n",
    "            word: The word for which to calculate the next word probability.\n",
    "            previous_words: A tuple containing the two previous words.\n",
    "            unigram_counts: A dictionary with counts of unigrams.\n",
    "            bigram_counts: A dictionary with counts of bigrams.\n",
    "            trigram_counts: A dictionary with counts of trigrams.\n",
    "            fourgram_counts: A dictionary with counts of fourgrams.\n",
    "            k: Smoothing parameter (default is 1).\n",
    "            lambda1: Weight for unigram model.\n",
    "            lambda2: Weight for bigram model.\n",
    "            lambda3: Weight for trigram model.\n",
    "            lambda4: Weight for fourgram model.\n",
    "\n",
    "        Returns:\n",
    "            The estimated probability of 'word' being the next word after 'previous_words' using linear interpolation with add-k smoothing.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create the unigram, bigram, trigram, and fourgram tuples\n",
    "        uni_gram = (word,)\n",
    "        bi_gram = (given_tri_gram[2], word)\n",
    "        tri_gram = (given_tri_gram[1], given_tri_gram[2], word)\n",
    "        four_gram = (given_tri_gram[0], given_tri_gram[1], given_tri_gram[2], word)\n",
    "\n",
    "        # Calculate probabilities for each n-gram model with add-k smoothing\n",
    "        unigram_prob = self.unigram_addk_probability(\n",
    "            current_uni=uni_gram, \n",
    "            k=self.k,\n",
    "        )\n",
    "        \n",
    "        bigram_prob = self.n_gram_addk_probability(\n",
    "            word=bi_gram[1], \n",
    "            given_gram=bi_gram[:1], \n",
    "            freq_previous=self.freq_uni, \n",
    "            freq_current=self.freq_bi,\n",
    "            k=self.k,\n",
    "        )\n",
    "        \n",
    "        trigram_prob = self.n_gram_addk_probability(\n",
    "            word=tri_gram[2],\n",
    "            given_gram=tri_gram[:2],\n",
    "            freq_previous=self.freq_bi, \n",
    "            freq_current=self.freq_tri,\n",
    "            k=self.k,\n",
    "        )\n",
    "        \n",
    "        fourgram_prob = self.n_gram_addk_probability(\n",
    "            word=four_gram[3], \n",
    "            given_gram=(four_gram[:3]), \n",
    "            freq_previous=self.freq_tri, \n",
    "            freq_current=self.freq_four,\n",
    "            k=self.k,\n",
    "        )\n",
    "\n",
    "        # Calculate interpolated probability\n",
    "        probability = (self.lambda1*unigram_prob) + (self.lambda2*bigram_prob) + (self.lambda3*trigram_prob) + (self.lambda4*fourgram_prob)\n",
    "\n",
    "        return probability\n",
    "    \n",
    "    # ---------- Add-k probability of unigram ----------\n",
    "    def unigram_addk_probability(\n",
    "        self,\n",
    "        current_uni: tuple,\n",
    "        k = 0.1\n",
    "    ):\n",
    "        uni_gram_count = self.freq_uni.get(current_uni, 0)\n",
    "        n_total_words = len(text_preprocessor.training_data)\n",
    "        n_unique_words = len(self.freq_uni)\n",
    "        \n",
    "        probability = (uni_gram_count + k) / (n_total_words + n_unique_words * k) \n",
    "        return probability\n",
    "    \n",
    "    # ---------- Add-k probability of n-gram, starting from bi-gram ----------\n",
    "    def n_gram_addk_probability(\n",
    "        self,\n",
    "        word: str,\n",
    "        given_gram: tuple,\n",
    "        freq_previous: dict, \n",
    "        freq_current: dict, \n",
    "        k = 0.1\n",
    "    ):\n",
    "        # new n-gram\n",
    "        n_gram = list(given_gram)\n",
    "        n_gram.append(word)\n",
    "        n_gram = tuple(n_gram)\n",
    "        \n",
    "        current_gram_count = freq_current.get(n_gram, 0)\n",
    "        previous_gram_count = freq_previous.get(given_gram, 0)\n",
    "        unique_word_count = len(self.freq_uni)\n",
    "        \n",
    "        probability = (current_gram_count + k) / (previous_gram_count + unique_word_count * k)\n",
    "        \n",
    "        return probability\n",
    "    \n",
    "    # ---------- Predict the next word ----------\n",
    "    def predict(\n",
    "        self,\n",
    "        previous_word: tuple[str, str, str],\n",
    "    ):\n",
    "        predictions = []\n",
    "\n",
    "        for word in self.freq_uni.keys():\n",
    "            probability = self.probability(word[0], previous_word)\n",
    "\n",
    "            predictions.append((word, probability)) \n",
    "\n",
    "        predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "        return predictions[0][0][0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = InterpolationAddK()\n",
    "model.predict(('this', 'is', 'a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_addk_probability(\n",
    "    freq_uni: dict,\n",
    "    current_uni: tuple,\n",
    "    k = 0.1\n",
    "):\n",
    "    \n",
    "    def get_n_total_words():\n",
    "        # tokenized_sent = tokenize_sentences(sentences)\n",
    "        return len(text_preprocessor.training_data)\n",
    "    \n",
    "    uni_gram_count = freq_uni.get(current_uni, 0)\n",
    "    n_total_words = get_n_total_words()\n",
    "    n_unique_words = len(freq_uni)\n",
    "    \n",
    "    probability = (uni_gram_count + k) / (n_total_words + n_unique_words * k) \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram_addk_probability(\n",
    "    word: str,\n",
    "    given_gram: tuple,\n",
    "    freq_uni: dict,\n",
    "    freq_previous: dict, \n",
    "    freq_current: dict, \n",
    "    k = 0.1\n",
    "):\n",
    "    # print(f'p({word} | {given_gram})', end=' = ')\n",
    "    # new n-gram\n",
    "    n_gram = list(given_gram)\n",
    "    n_gram.append(word)\n",
    "    n_gram = tuple(n_gram)\n",
    "    \n",
    "    current_gram_count = freq_current.get(n_gram, 0)\n",
    "    previous_gram_count = freq_previous.get(given_gram, 0)\n",
    "    unique_word_count = len(freq_uni)\n",
    "    \n",
    "    probability = (current_gram_count + k) / (previous_gram_count + unique_word_count * k)\n",
    "    \n",
    "    # print(f'{probability}')\n",
    "    \n",
    "    # print(f'current:{n_gram}: {current_gram_count}')\n",
    "    # print(f'previous: {given_gram}: {previous_gram_count}')\n",
    "    # print(unique_word_count)\n",
    "    \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def probability(\n",
    "    word: str,\n",
    "    previous_tri_gram: tuple,\n",
    "    freq_uni: dict,\n",
    "    freq_bi: dict,\n",
    "    freq_tri: dict,\n",
    "    freq_four: dict,\n",
    "    k=0.1,  # Smoothing parameter\n",
    "    lambda1=0.1,\n",
    "    lambda2=0.2,\n",
    "    lambda3=0.3,\n",
    "    lambda4=0.4,\n",
    "):\n",
    "    \"\"\"\n",
    "    Estimate the probability of a word being the next word after given previous words using linear interpolation with add-k smoothing.\n",
    "\n",
    "    Args:\n",
    "        word: The word for which to calculate the next word probability.\n",
    "        previous_words: A tuple containing the two previous words.\n",
    "        unigram_counts: A dictionary with counts of unigrams.\n",
    "        bigram_counts: A dictionary with counts of bigrams.\n",
    "        trigram_counts: A dictionary with counts of trigrams.\n",
    "        fourgram_counts: A dictionary with counts of fourgrams.\n",
    "        k: Smoothing parameter (default is 1).\n",
    "        lambda1: Weight for unigram model.\n",
    "        lambda2: Weight for bigram model.\n",
    "        lambda3: Weight for trigram model.\n",
    "        lambda4: Weight for fourgram model.\n",
    "\n",
    "    Returns:\n",
    "        The estimated probability of 'word' being the next word after 'previous_words' using linear interpolation with add-k smoothing.\n",
    "    \"\"\"\n",
    "    # Create the unigram, bigram, trigram, and fourgram tuples\n",
    "    uni_gram = (word,)\n",
    "    bi_gram = (previous_tri_gram[2], word)\n",
    "    tri_gram = (previous_tri_gram[1], previous_tri_gram[2], word)\n",
    "    four_gram = (previous_tri_gram[0], previous_tri_gram[1], previous_tri_gram[2], word)\n",
    "    \n",
    "    # print(uni_gram)\n",
    "    # print(bi_gram)\n",
    "    # print(tri_gram)\n",
    "    # print(four_gram)\n",
    "\n",
    "    # Calculate probabilities for each n-gram model with add-k smoothing\n",
    "    unigram_prob = unigram_addk_probability(\n",
    "        freq_uni=freq_uni, \n",
    "        current_uni=uni_gram, \n",
    "        k=k,\n",
    "    )\n",
    "    \n",
    "    bigram_prob = n_gram_addk_probability(\n",
    "        word=bi_gram[1], \n",
    "        given_gram=bi_gram[:1], \n",
    "        freq_uni=freq_uni, \n",
    "        freq_previous=freq_uni, \n",
    "        freq_current=freq_bi,\n",
    "        k=k,\n",
    "    )\n",
    "    \n",
    "    trigram_prob = n_gram_addk_probability(\n",
    "        word=tri_gram[2],\n",
    "        given_gram=tri_gram[:2],\n",
    "        freq_uni=freq_uni, \n",
    "        freq_previous=freq_bi, \n",
    "        freq_current=freq_tri,\n",
    "        k=k,\n",
    "    )\n",
    "    \n",
    "    fourgram_prob = n_gram_addk_probability(\n",
    "        word=four_gram[3], \n",
    "        given_gram=(four_gram[:3]),\n",
    "        freq_uni=freq_uni, \n",
    "        freq_previous=freq_tri, \n",
    "        freq_current=freq_four,\n",
    "        k=k,\n",
    "    )\n",
    "    \n",
    "    # print(len(freq_uni))\n",
    "    # print(len(freq_tri))\n",
    "    # print(len(freq_four))\n",
    "    \n",
    "    # print(f'uni_prob: {unigram_prob}')\n",
    "    # print(f'bi_prob: {bigram_prob}')\n",
    "    # print(f'tri_prob: {trigram_prob}')\n",
    "    # print(f'four_prob: {fourgram_prob}')\n",
    "\n",
    "    # Calculate interpolated probability\n",
    "    probability = (lambda1*unigram_prob) + (lambda2*bigram_prob) + (lambda3*trigram_prob) + (lambda4*fourgram_prob)\n",
    "    print(f'probability: {probability}')\n",
    "\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability: 0.8235294117647058\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8235294117647058"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability(\n",
    "    'test', \n",
    "    ('this', 'is', 'a'),\n",
    "    text_preprocessor.freq_uni,\n",
    "    text_preprocessor.freq_bi,\n",
    "    text_preprocessor.freq_tri,\n",
    "    text_preprocessor.freq_four,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a model using interpolation\n",
    "def language_model_interpolation(\n",
    "    previous_word: tuple[str, str, str],\n",
    "    freq_uni: dict[tuple, int],\n",
    "    freq_bi: dict[tuple, int],\n",
    "    freq_tri: dict[tuple, int],\n",
    "    freq_four: dict[tuple, int],\n",
    "):\n",
    "    predictions = []\n",
    "\n",
    "    for word in freq_uni.keys():\n",
    "        probability = probability(word[0], previous_word, freq_uni, freq_bi, freq_tri, freq_four,)\n",
    "\n",
    "        predictions.append((word, probability)) \n",
    "\n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    return predictions[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'probability' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m next_word \u001b[38;5;241m=\u001b[39m language_model_interpolation((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthis\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m), text_preprocessor\u001b[38;5;241m.\u001b[39mfreq_uni, text_preprocessor\u001b[38;5;241m.\u001b[39mfreq_bi, text_preprocessor\u001b[38;5;241m.\u001b[39mfreq_tri, text_preprocessor\u001b[38;5;241m.\u001b[39mfreq_four)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNext word: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m next_word)\n",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m, in \u001b[0;36mlanguage_model_interpolation\u001b[0;34m(previous_word, freq_uni, freq_bi, freq_tri, freq_four)\u001b[0m\n\u001b[1;32m      9\u001b[0m predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m freq_uni\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m---> 12\u001b[0m     probability \u001b[38;5;241m=\u001b[39m probability(word[\u001b[38;5;241m0\u001b[39m], previous_word, freq_uni, freq_bi, freq_tri, freq_four,)\n\u001b[1;32m     14\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mappend((word, probability)) \n\u001b[1;32m     16\u001b[0m predictions\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'probability' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "next_word = language_model_interpolation(('this', 'is', 'a'), text_preprocessor.freq_uni, text_preprocessor.freq_bi, text_preprocessor.freq_tri, text_preprocessor.freq_four)\n",
    "print(\"Next word: \" + next_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preprocessor.freq_bi.get(('is', 'test'),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "print(n_gram_addk_probability('test', ('a',), text_preprocessor.freq_uni, text_preprocessor.freq_uni, text_preprocessor.freq_bi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6470588235294118"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1 + 0.1) / (1 + 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2352941176470587"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_addk_probability(text_preprocessor.freq_uni, ('this',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14285714285714288"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1 + 0.1) / (7 + 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([(('<s>', 'this', 'is', 'a'), 1), (('this', 'is', 'a', 'test'), 2), (('is', 'a', 'test', 'corpus'), 2), (('a', 'test', 'corpus', 'this'), 1), (('test', 'corpus', 'this', 'is'), 1), (('corpus', 'this', 'is', 'a'), 1), (('a', 'test', 'corpus', '</s>'), 1)])\n",
      "probability: 0.8235294117647058\n",
      "0.8235294117647058\n"
     ]
    }
   ],
   "source": [
    "print(text_preprocessor.freq_four.items())\n",
    "print(probability('test', ('this', 'is', 'a'), text_preprocessor.freq_uni, text_preprocessor.freq_bi, text_preprocessor.freq_tri, text_preprocessor.freq_four,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity_interpolation():\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
