{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the preprocessed data from the preprocess file\n",
    "from preprocess import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unigram_addk_probability(\n",
    "    freq_uni: dict,\n",
    "    current_uni: tuple,\n",
    "    k = 0.1\n",
    "):\n",
    "    \n",
    "    def get_n_total_words():\n",
    "        # tokenized_sent = tokenize_sentences(sentences)\n",
    "        return len(tokenized_words['sentences'])\n",
    "    \n",
    "    uni_gram_count = freq_uni.get(current_uni, 0)\n",
    "    n_total_words = get_n_total_words()\n",
    "    n_unique_words = len(freq_uni)\n",
    "    \n",
    "    probability = (uni_gram_count + k) / (n_total_words + n_unique_words * k) \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram_addk_probability(\n",
    "    word: str,\n",
    "    given_gram: tuple,\n",
    "    freq_uni: dict,\n",
    "    freq_previous: dict, \n",
    "    freq_current: dict, \n",
    "    k = 0.1\n",
    "):\n",
    "    print(f'p({word} | {given_gram})', end=' = ')\n",
    "    # new n-gram\n",
    "    n_gram = list(given_gram)\n",
    "    n_gram.append(word)\n",
    "    n_gram = tuple(n_gram)\n",
    "    \n",
    "    current_gram_count = freq_current.get(n_gram, 0)\n",
    "    previous_gram_count = freq_previous.get(given_gram, 0)\n",
    "    unique_word_count = len(freq_uni)\n",
    "    \n",
    "    probability = (current_gram_count + k) / (previous_gram_count + unique_word_count * k)\n",
    "    \n",
    "    print(f'{probability}')\n",
    "    \n",
    "    print(f'current:{n_gram}: {current_gram_count}')\n",
    "    print(f'previous: {given_gram}: {previous_gram_count}')\n",
    "    print(unique_word_count)\n",
    "    \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def addk_probability_interpolation(\n",
    "    word: str,\n",
    "    previous_tri_gram: tuple,\n",
    "    freq_uni: dict,\n",
    "    freq_bi: dict,\n",
    "    freq_tri: dict,\n",
    "    freq_four: dict,\n",
    "    k=0.1,  # Smoothing parameter\n",
    "    lambda1=0.1,\n",
    "    lambda2=0.2,\n",
    "    lambda3=0.3,\n",
    "    lambda4=0.4,\n",
    "):\n",
    "    \"\"\"\n",
    "    Estimate the probability of a word being the next word after given previous words using linear interpolation with add-k smoothing.\n",
    "\n",
    "    Args:\n",
    "        word: The word for which to calculate the next word probability.\n",
    "        previous_words: A tuple containing the two previous words.\n",
    "        unigram_counts: A dictionary with counts of unigrams.\n",
    "        bigram_counts: A dictionary with counts of bigrams.\n",
    "        trigram_counts: A dictionary with counts of trigrams.\n",
    "        fourgram_counts: A dictionary with counts of fourgrams.\n",
    "        k: Smoothing parameter (default is 1).\n",
    "        lambda1: Weight for unigram model.\n",
    "        lambda2: Weight for bigram model.\n",
    "        lambda3: Weight for trigram model.\n",
    "        lambda4: Weight for fourgram model.\n",
    "\n",
    "    Returns:\n",
    "        The estimated probability of 'word' being the next word after 'previous_words' using linear interpolation with add-k smoothing.\n",
    "    \"\"\"\n",
    "    # Create the unigram, bigram, trigram, and fourgram tuples\n",
    "    uni_gram = (word,)\n",
    "    bi_gram = (previous_tri_gram[2], word)\n",
    "    tri_gram = (previous_tri_gram[1], previous_tri_gram[2], word)\n",
    "    four_gram = (previous_tri_gram[0], previous_tri_gram[1], previous_tri_gram[2], word)\n",
    "    \n",
    "    print(uni_gram)\n",
    "    print(bi_gram)\n",
    "    print(tri_gram)\n",
    "    print(four_gram)\n",
    "\n",
    "    # Calculate probabilities for each n-gram model with add-k smoothing\n",
    "    unigram_prob = unigram_addk_probability(\n",
    "        freq_uni=freq_uni, \n",
    "        current_uni=uni_gram, \n",
    "        k=k,\n",
    "    )\n",
    "    \n",
    "    bigram_prob = n_gram_addk_probability(\n",
    "        word=bi_gram[1], \n",
    "        given_gram=bi_gram[:1], \n",
    "        freq_uni=freq_uni, \n",
    "        freq_previous=freq_uni, \n",
    "        freq_current=freq_bi,\n",
    "        k=k,\n",
    "    )\n",
    "    \n",
    "    trigram_prob = n_gram_addk_probability(\n",
    "        word=tri_gram[2],\n",
    "        given_gram=tri_gram[:2],\n",
    "        freq_uni=freq_uni, \n",
    "        freq_previous=freq_bi, \n",
    "        freq_current=freq_tri,\n",
    "        k=k,\n",
    "    )\n",
    "    \n",
    "    fourgram_prob = n_gram_addk_probability(\n",
    "        word=four_gram[3], \n",
    "        given_gram=(four_gram[:3]),\n",
    "        freq_uni=freq_uni, \n",
    "        freq_previous=freq_tri, \n",
    "        freq_current=freq_four,\n",
    "        k=k,\n",
    "    )\n",
    "    \n",
    "    # print(len(freq_uni))\n",
    "    # print(len(freq_tri))\n",
    "    # print(len(freq_four))\n",
    "    \n",
    "    print(f'uni_prob: {unigram_prob}')\n",
    "    print(f'bi_prob: {bigram_prob}')\n",
    "    print(f'tri_prob: {trigram_prob}')\n",
    "    print(f'four_prob: {fourgram_prob}')\n",
    "\n",
    "    # Calculate interpolated probability\n",
    "    probability = (lambda1*unigram_prob) + (lambda2*bigram_prob) + (lambda3*trigram_prob) + (lambda4*fourgram_prob)\n",
    "\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('test',)\n",
      "('a', 'test')\n",
      "('is', 'a', 'test')\n",
      "('this', 'is', 'a', 'test')\n",
      "p(test | ('a',)) = 0.6470588235294118\n",
      "current:('a', 'test'): 1\n",
      "previous: ('a',): 1\n",
      "7\n",
      "p(test | ('is', 'a')) = 0.6470588235294118\n",
      "current:('is', 'a', 'test'): 1\n",
      "previous: ('is', 'a'): 1\n",
      "7\n",
      "p(test | ('this', 'is', 'a')) = 0.6470588235294118\n",
      "current:('this', 'is', 'a', 'test'): 1\n",
      "previous: ('this', 'is', 'a'): 1\n",
      "7\n",
      "uni_prob: 0.14285714285714288\n",
      "bi_prob: 0.6470588235294118\n",
      "tri_prob: 0.6470588235294118\n",
      "four_prob: 0.6470588235294118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.596638655462185"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addk_probability_interpolation(\n",
    "    'test', \n",
    "    ('this', 'is', 'a'),\n",
    "    freq_uni,\n",
    "    freq_bi,\n",
    "    freq_tri,\n",
    "    freq_four,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_bi.get(('is', 'test'),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "5\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(freq_uni))\n",
    "print(len(freq_tri))\n",
    "print(len(freq_four))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(test | ('a',)) = 0.6470588235294118\n",
      "current:('a', 'test'): 1\n",
      "previous: ('a',): 1\n",
      "7\n",
      "0.6470588235294118\n"
     ]
    }
   ],
   "source": [
    "print(n_gram_addk_probability('test', ('a',), freq_uni, freq_uni, freq_bi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6470588235294118"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1 + 0.1) / (1 + 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14285714285714288"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_addk_probability(freq_uni, ('this',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14285714285714288"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1 + 0.1) / (7 + 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigram_addk_probability(freq_uni, ('iNtelligence'.lower(),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# unigram_counts = Counter(unigrams)\n",
    "# bigram_counts = Counter(bigrams)\n",
    "# trigram_counts = Counter(trigrams)\n",
    "# fourgram_counts = Counter(fourgrams)\n",
    "\n",
    "\n",
    "# test = addk_probability_interpolation(\n",
    "#     \"it\",\n",
    "#     (\"for\", \"testing\", \"purposes\"),\n",
    "#     unigram_counts,\n",
    "#     bigram_counts,\n",
    "#     trigram_counts,\n",
    "#     fourgram_counts,\n",
    "# )\n",
    "\n",
    "# print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_text(\n",
    "#     seed_words,\n",
    "#     length,\n",
    "#     interpolation_function,\n",
    "#     unigram_counts,\n",
    "#     bigram_counts,\n",
    "#     trigram_counts,\n",
    "#     fourgram_counts,\n",
    "# ):\n",
    "#     generated_text = list(seed_words)\n",
    "\n",
    "#     for _ in range(length):\n",
    "#         previous_words = tuple(generated_text[-3:])  # Get the last three words\n",
    "#         next_word = generate_next_word(\n",
    "#             interpolation_function,\n",
    "#             previous_words,\n",
    "#             unigram_counts,\n",
    "#             bigram_counts,\n",
    "#             trigram_counts,\n",
    "#             fourgram_counts,\n",
    "#         )\n",
    "#         generated_text.append(next_word)\n",
    "\n",
    "#     return \" \".join(generated_text)\n",
    "\n",
    "\n",
    "# def generate_next_word(\n",
    "#     interpolation_function,\n",
    "#     previous_words,\n",
    "#     unigram_counts,\n",
    "#     bigram_counts,\n",
    "#     trigram_counts,\n",
    "#     fourgram_counts,\n",
    "# ):\n",
    "#     # Create a list to store predicted words and their probabilities\n",
    "#     predictions = []\n",
    "\n",
    "#     # Iterate through all possible unigrams\n",
    "#     for word in unigram_counts.keys():\n",
    "#         # Calculate the probability of the word being the next word\n",
    "#         probability = interpolation_function(\n",
    "#             word,\n",
    "#             previous_words,\n",
    "#             unigram_counts,\n",
    "#             bigram_counts,\n",
    "#             trigram_counts,\n",
    "#             fourgram_counts,\n",
    "#         )\n",
    "\n",
    "#         # Add the word and its probability to the list\n",
    "#         predictions.append((word, probability))\n",
    "\n",
    "#     # Sort the predictions by probability in descending order\n",
    "#     predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#     # Return the word with the highest probability\n",
    "#     return str(predictions[0][0])\n",
    "#     # return predictions[0][0]\n",
    "\n",
    "\n",
    "# seed_words = [\"for\", \"testing\", \"purposes\",]  # Seed sequence\n",
    "# generated_text = generate_text(\n",
    "#     seed_words,\n",
    "#     100,\n",
    "#     addk_probability_interpolation,\n",
    "#     unigram_counts,\n",
    "#     bigram_counts,\n",
    "#     trigram_counts,\n",
    "#     fourgram_counts,\n",
    "# )\n",
    "# print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a model using interpolation\n",
    "def language_model_interpolation():\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity_interpolation():\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
