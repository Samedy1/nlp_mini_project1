{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the preprocessed data from the preprocess file\n",
    "from preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of the file\n",
    "# with open(\"corpus/test.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "#     data = file.read()\n",
    "\n",
    "# # Split the content into sentences using \"\\n\" as the delimiter\n",
    "# sentences = data.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import string\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.util import ngrams\n",
    "\n",
    "\n",
    "# def tokenize_sentences(sentences):\n",
    "#     unigrams = []\n",
    "#     bigrams = []\n",
    "#     trigrams = []\n",
    "#     fourgrams = []\n",
    "\n",
    "#     emoji_pattern = re.compile(\n",
    "#         \"[\"\n",
    "#         \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "#         \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "#         \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "#         \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "#         \"\\U00002702-\\U000027B0\"\n",
    "#         \"\\U000024C2-\\U0001F251\"\n",
    "#         \"]+\",\n",
    "#         flags=re.UNICODE,\n",
    "#     )\n",
    "\n",
    "#     for sentence in sentences:\n",
    "#         # Convert to lowercase\n",
    "#         sentence = sentence.lower()\n",
    "\n",
    "#         # Remove emojis\n",
    "#         sentence = emoji_pattern.sub(r\"\", sentence)\n",
    "\n",
    "#         # Tokenize the sentence\n",
    "#         tokens = word_tokenize(sentence)\n",
    "\n",
    "#         # Remove punctuation\n",
    "#         tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "#         # Generate unigrams\n",
    "#         unigrams.extend(tokens)\n",
    "\n",
    "#         # Generate bigrams\n",
    "#         bigrams.extend(ngrams(tokens, 2))\n",
    "\n",
    "#         # Generate trigrams\n",
    "#         trigrams.extend(ngrams(tokens, 3))\n",
    "\n",
    "#         # Generate fourgrams\n",
    "#         fourgrams.extend(ngrams(tokens, 4))\n",
    "\n",
    "#     return unigrams, bigrams, trigrams, fourgrams\n",
    "\n",
    "\n",
    "# sentences = [\n",
    "#     \"This is a sample sentence.\",\n",
    "#     \"Another example sentence here.\",\n",
    "#     \"And one more sentence for testing purposes.\",\n",
    "# ]\n",
    "\n",
    "# unigrams, bigrams, trigrams, fourgrams = tokenize_sentences(sentences)\n",
    "\n",
    "# print(\"Unigrams:\", unigrams[:10])\n",
    "# print(\"Bigrams:\", list(bigrams)[:10])\n",
    "# print(\"Trigrams:\", list(trigrams)[:10])\n",
    "# print(\"Fourgrams:\", list(fourgrams)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, value in freq_four.items():\n",
    "#     print(count());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq_uni.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unigram_addk_probability(\n",
    "    freq_uni: dict,\n",
    "    current_uni: tuple,\n",
    "    k = 0.1\n",
    "):\n",
    "    \n",
    "    def get_n_total_words():\n",
    "        # tokenized_sent = tokenize_sentences(sentences)\n",
    "        return len(tokenized_words['sentences'])\n",
    "    \n",
    "    uni_gram_count = freq_uni.get(current_uni, 0)\n",
    "    n_total_words = get_n_total_words()\n",
    "    n_unique_words = len(freq_uni)\n",
    "    \n",
    "    probability = (uni_gram_count + k) / (n_total_words + n_unique_words * k) \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram_addk_probability(\n",
    "    word: str,\n",
    "    given_gram: tuple,\n",
    "    freq_uni: dict,\n",
    "    freq_previous: dict, \n",
    "    freq_current: dict, \n",
    "    k = 0.1\n",
    "):\n",
    "    \n",
    "    # new n-gram\n",
    "    n_gram = list(given_gram)\n",
    "    n_gram.append(word)\n",
    "    n_gram = tuple(n_gram)\n",
    "    \n",
    "    current_gram_count = freq_current.get(n_gram, 0)\n",
    "    previous_gram_count = freq_previous.get(given_gram, 0)\n",
    "    unique_word_count = len(freq_uni)\n",
    "    # print(current_gram_count)\n",
    "    # print(previous_gram_count)\n",
    "    # print(unique_word_count)\n",
    "    \n",
    "    return (current_gram_count + k) / (previous_gram_count + unique_word_count * k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def addk_probability_interpolation(\n",
    "    word: str,\n",
    "    previous_tri_gram: tuple,\n",
    "    freq_uni: dict,\n",
    "    freq_bi: dict,\n",
    "    freq_tri: dict,\n",
    "    freq_four: dict,\n",
    "    k=0.1,  # Smoothing parameter\n",
    "    lambda1=0.1,\n",
    "    lambda2=0.2,\n",
    "    lambda3=0.3,\n",
    "    lambda4=0.4,\n",
    "):\n",
    "    \"\"\"\n",
    "    Estimate the probability of a word being the next word after given previous words using linear interpolation with add-k smoothing.\n",
    "\n",
    "    Args:\n",
    "        word: The word for which to calculate the next word probability.\n",
    "        previous_words: A tuple containing the two previous words.\n",
    "        unigram_counts: A dictionary with counts of unigrams.\n",
    "        bigram_counts: A dictionary with counts of bigrams.\n",
    "        trigram_counts: A dictionary with counts of trigrams.\n",
    "        fourgram_counts: A dictionary with counts of fourgrams.\n",
    "        k: Smoothing parameter (default is 1).\n",
    "        lambda1: Weight for unigram model.\n",
    "        lambda2: Weight for bigram model.\n",
    "        lambda3: Weight for trigram model.\n",
    "        lambda4: Weight for fourgram model.\n",
    "\n",
    "    Returns:\n",
    "        The estimated probability of 'word' being the next word after 'previous_words' using linear interpolation with add-k smoothing.\n",
    "    \"\"\"\n",
    "    # Create the unigram, bigram, trigram, and fourgram tuples\n",
    "    uni_gram = (word,)\n",
    "    bi_gram = (previous_tri_gram[1], word)\n",
    "    tri_gram = (previous_tri_gram[0], previous_tri_gram[1], word)\n",
    "    four_gram = (previous_tri_gram[0], previous_tri_gram[1], previous_tri_gram[2], word)\n",
    "\n",
    "    # Calculate probabilities for each n-gram model with add-k smoothing\n",
    "    unigram_prob = unigram_addk_probability(\n",
    "        freq_uni=freq_uni, \n",
    "        current_uni=uni_gram, \n",
    "        k=k,\n",
    "    )\n",
    "    \n",
    "    bigram_prob = n_gram_addk_probability(\n",
    "        word=bi_gram[1], \n",
    "        given_gram=bi_gram[:1], \n",
    "        freq_uni=freq_uni, \n",
    "        freq_previous=freq_uni, \n",
    "        freq_current=freq_bi,\n",
    "        k=k,\n",
    "    )\n",
    "    \n",
    "    trigram_prob = n_gram_addk_probability(\n",
    "        word=tri_gram[2],\n",
    "        given_gram=tri_gram[:2],\n",
    "        freq_uni=freq_uni, \n",
    "        freq_previous=freq_bi, \n",
    "        freq_current=freq_tri,\n",
    "        k=k,\n",
    "    )\n",
    "    \n",
    "    fourgram_prob = n_gram_addk_probability(\n",
    "        word=four_gram[3], \n",
    "        given_gram=four_gram[:3],\n",
    "        freq_uni=freq_uni, \n",
    "        freq_previous=freq_tri, \n",
    "        freq_current=freq_four,\n",
    "        k=k,\n",
    "    )\n",
    "    \n",
    "    print(unigram_prob)\n",
    "    print(bigram_prob)\n",
    "    print(trigram_prob)\n",
    "    print(fourgram_prob)\n",
    "\n",
    "    # Calculate interpolated probability\n",
    "    probability = (lambda1*unigram_prob) + (lambda2*bigram_prob) + (lambda3*trigram_prob) + (lambda4*fourgram_prob)\n",
    "\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14285714285714288\n",
      "0.058823529411764705\n",
      "0.058823529411764705\n",
      "0.6470588235294118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3025210084033614"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addk_probability_interpolation(\n",
    "    'test', \n",
    "    ('this', 'is', 'a'),\n",
    "    freq_uni,\n",
    "    freq_bi,\n",
    "    freq_tri,\n",
    "    freq_four,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14285714285714285"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gram_addk_probability('is', ('this' 'is'), freq_uni, freq_uni, freq_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6470588235294118"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1 + 0.1) / (1 + 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14285714285714288"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_addk_probability(freq_uni, ('this',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14285714285714288"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1 + 0.1) / (7 + 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012987012987012988"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_addk_probability(freq_uni, ('iNtelligence'.lower(),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unigrams' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[0;32m----> 2\u001b[0m unigram_counts \u001b[38;5;241m=\u001b[39m Counter(unigrams)\n\u001b[1;32m      3\u001b[0m bigram_counts \u001b[38;5;241m=\u001b[39m Counter(bigrams)\n\u001b[1;32m      4\u001b[0m trigram_counts \u001b[38;5;241m=\u001b[39m Counter(trigrams)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'unigrams' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "unigram_counts = Counter(unigrams)\n",
    "bigram_counts = Counter(bigrams)\n",
    "trigram_counts = Counter(trigrams)\n",
    "fourgram_counts = Counter(fourgrams)\n",
    "\n",
    "\n",
    "test = addk_probability_interpolation(\n",
    "    \"it\",\n",
    "    (\"for\", \"testing\", \"purposes\"),\n",
    "    unigram_counts,\n",
    "    bigram_counts,\n",
    "    trigram_counts,\n",
    "    fourgram_counts,\n",
    ")\n",
    "\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for testing purposes purposes this this is is a a sample sample sentence sentence here here this this is is a a sample sample sentence sentence here here this this is is a a sample sample sentence sentence here here this this is is a a sample sample sentence sentence here here this this is is a a sample sample sentence sentence here here this this is is a a sample sample sentence sentence here here this this is is a a sample sample sentence sentence here here this this is is a a sample sample sentence sentence here here this this is\n"
     ]
    }
   ],
   "source": [
    "def generate_text(\n",
    "    seed_words,\n",
    "    length,\n",
    "    interpolation_function,\n",
    "    unigram_counts,\n",
    "    bigram_counts,\n",
    "    trigram_counts,\n",
    "    fourgram_counts,\n",
    "):\n",
    "    generated_text = list(seed_words)\n",
    "\n",
    "    for _ in range(length):\n",
    "        previous_words = tuple(generated_text[-3:])  # Get the last three words\n",
    "        next_word = generate_next_word(\n",
    "            interpolation_function,\n",
    "            previous_words,\n",
    "            unigram_counts,\n",
    "            bigram_counts,\n",
    "            trigram_counts,\n",
    "            fourgram_counts,\n",
    "        )\n",
    "        generated_text.append(next_word)\n",
    "\n",
    "    return \" \".join(generated_text)\n",
    "\n",
    "\n",
    "def generate_next_word(\n",
    "    interpolation_function,\n",
    "    previous_words,\n",
    "    unigram_counts,\n",
    "    bigram_counts,\n",
    "    trigram_counts,\n",
    "    fourgram_counts,\n",
    "):\n",
    "    # Create a list to store predicted words and their probabilities\n",
    "    predictions = []\n",
    "\n",
    "    # Iterate through all possible unigrams\n",
    "    for word in unigram_counts.keys():\n",
    "        # Calculate the probability of the word being the next word\n",
    "        probability = interpolation_function(\n",
    "            word,\n",
    "            previous_words,\n",
    "            unigram_counts,\n",
    "            bigram_counts,\n",
    "            trigram_counts,\n",
    "            fourgram_counts,\n",
    "        )\n",
    "\n",
    "        # Add the word and its probability to the list\n",
    "        predictions.append((word, probability))\n",
    "\n",
    "    # Sort the predictions by probability in descending order\n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Return the word with the highest probability\n",
    "    return str(predictions[0][0])\n",
    "    # return predictions[0][0]\n",
    "\n",
    "\n",
    "seed_words = [\"for\", \"testing\", \"purposes\",]  # Seed sequence\n",
    "generated_text = generate_text(\n",
    "    seed_words,\n",
    "    100,\n",
    "    addk_probability_interpolation,\n",
    "    unigram_counts,\n",
    "    bigram_counts,\n",
    "    trigram_counts,\n",
    "    fourgram_counts,\n",
    ")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a model using interpolation\n",
    "def language_model_interpolation():\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity_interpolation():\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
