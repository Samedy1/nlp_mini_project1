{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import *\n",
    "\n",
    "text_preprocessor = TextPreprocessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpolationAddK:\n",
    "    def __init__(\n",
    "        self,\n",
    "        k=0.1,  # Smoothing parameter\n",
    "        lambda1=0.1,\n",
    "        lambda2=0.2,\n",
    "        lambda3=0.3,\n",
    "        lambda4=0.4,\n",
    "    ) -> None:\n",
    "        # preprocess\n",
    "        text_preprocessor = TextPreprocessor()\n",
    "        \n",
    "        # initialize necessary fields\n",
    "        self.freq_uni = text_preprocessor.freq_uni\n",
    "        self.freq_bi = text_preprocessor.freq_bi\n",
    "        self.freq_tri = text_preprocessor.freq_tri\n",
    "        self.freq_four = text_preprocessor.freq_four\n",
    "        \n",
    "        # k and lambda\n",
    "        self.k = k\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        self.lambda3 = lambda3\n",
    "        self.lambda4 = lambda4\n",
    "    \n",
    "    # ---------- interpolation with Add-k probability of unigram ----------\n",
    "    def probability(\n",
    "        self,\n",
    "        word: str,\n",
    "        given_tri_gram: tuple,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Estimate the probability of a word being the next word after given previous words using linear interpolation with add-k smoothing.\n",
    "\n",
    "        Args:\n",
    "            word: The word for which to calculate the next word probability.\n",
    "            give_tri_gram: A tuple containing the three previous words.\n",
    "\n",
    "        Returns:\n",
    "            The estimated probability of 'word' being the next word after 'given_tri_gram' using linear interpolation with add-k smoothing.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create the unigram, bigram, trigram, and fourgram tuples\n",
    "        uni_gram = (word,)\n",
    "        bi_gram = (given_tri_gram[2], word)\n",
    "        tri_gram = (given_tri_gram[1], given_tri_gram[2], word)\n",
    "        four_gram = (given_tri_gram[0], given_tri_gram[1], given_tri_gram[2], word)\n",
    "\n",
    "        # Calculate probabilities for each n-gram model with add-k smoothing\n",
    "        unigram_prob = self.unigram_addk_probability(\n",
    "            current_uni=uni_gram, \n",
    "            k=self.k,\n",
    "        )\n",
    "        \n",
    "        bigram_prob = self.n_gram_addk_probability(\n",
    "            word=bi_gram[1], \n",
    "            given_gram=bi_gram[:1], \n",
    "            freq_previous=self.freq_uni, \n",
    "            freq_current=self.freq_bi,\n",
    "            k=self.k,\n",
    "        )\n",
    "        \n",
    "        trigram_prob = self.n_gram_addk_probability(\n",
    "            word=tri_gram[2],\n",
    "            given_gram=tri_gram[:2],\n",
    "            freq_previous=self.freq_bi, \n",
    "            freq_current=self.freq_tri,\n",
    "            k=self.k,\n",
    "        )\n",
    "        \n",
    "        fourgram_prob = self.n_gram_addk_probability(\n",
    "            word=four_gram[3], \n",
    "            given_gram=(four_gram[:3]), \n",
    "            freq_previous=self.freq_tri, \n",
    "            freq_current=self.freq_four,\n",
    "            k=self.k,\n",
    "        )\n",
    "\n",
    "        # Calculate interpolated probability\n",
    "        probability = (self.lambda1*unigram_prob) + (self.lambda2*bigram_prob) + (self.lambda3*trigram_prob) + (self.lambda4*fourgram_prob)\n",
    "        \n",
    "        # print(f'probability of {word}: {probability}')\n",
    "\n",
    "        return probability\n",
    "    \n",
    "    # ---------- Add-k probability of unigram ----------\n",
    "    def unigram_addk_probability(\n",
    "        self,\n",
    "        current_uni: tuple,\n",
    "        k = 0.1\n",
    "    ):\n",
    "        uni_gram_count = self.freq_uni.get(current_uni, 0)\n",
    "        n_total_words = len(text_preprocessor.training_data)\n",
    "        n_unique_words = len(self.freq_uni)\n",
    "        \n",
    "        probability = (uni_gram_count + k) / (n_total_words + n_unique_words * k) \n",
    "        return probability\n",
    "    \n",
    "    # ---------- Add-k probability of n-gram, starting from bi-gram ----------\n",
    "    def n_gram_addk_probability(\n",
    "        self,\n",
    "        word: str,\n",
    "        given_gram: tuple,\n",
    "        freq_previous: dict, \n",
    "        freq_current: dict, \n",
    "        k = 0.1\n",
    "    ):\n",
    "        # new n-gram\n",
    "        n_gram = list(given_gram)\n",
    "        n_gram.append(word)\n",
    "        n_gram = tuple(n_gram)\n",
    "        \n",
    "        current_gram_count = freq_current.get(n_gram, 0)\n",
    "        previous_gram_count = freq_previous.get(given_gram, 0)\n",
    "        unique_word_count = len(self.freq_uni)\n",
    "        \n",
    "        probability = (current_gram_count + k) / (previous_gram_count + unique_word_count * k)\n",
    "        \n",
    "        return probability\n",
    "    \n",
    "    # ---------- Predict the next word ----------\n",
    "    def predict(\n",
    "        self,\n",
    "        previous_word: tuple[str, str, str],\n",
    "    ):\n",
    "        predictions = []\n",
    "        for word in self.freq_uni.keys():\n",
    "            # if (word[0] != '<s>' and word[0] != '</s>'):\n",
    "            probability = self.probability(word[0], previous_word)\n",
    "            predictions.append((word, probability)) \n",
    "\n",
    "        predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "        # print(predictions)\n",
    "        return predictions[0][0][0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = InterpolationAddK(\n",
    "    k=pow(10, -2), \n",
    "    lambda1=pow(10, -5), \n",
    "    lambda2=pow(10, -5),\n",
    "    lambda3=pow(10, -5),\n",
    "    lambda4=pow(10, -1),\n",
    ")\n",
    "model.predict(('<s>', 'computer', 'science'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation for Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0029782746726499\n"
     ]
    }
   ],
   "source": [
    "interpolation_model = InterpolationAddK()\n",
    "\n",
    "def perplexity_interpolation_for_val():\n",
    "    pp = 1\n",
    "    tokenized_validation = text_preprocessor.tokenize_words(text_preprocessor.validation_data)\n",
    "    tokens = tokenized_validation['sentences']\n",
    "    for i in range(3, len(tokens)):  # Adjusted loop range for fourgram\n",
    "        current_word = tokens[i]  # Adjusted indexing for fourgram\n",
    "        previous_four_gram = (tokens[i - 3], tokens[i - 2], tokens[i - 1], tokens[i])  # Construct fourgram tuple\n",
    "\n",
    "        probability = interpolation_model.probability(\n",
    "            word=current_word,\n",
    "            given_tri_gram=previous_four_gram[:-1],\n",
    "            \n",
    "        )  # Calculate probability\n",
    "\n",
    "        pp *= (1 / probability) ** (1 / len(tokens))  # Update perplexity\n",
    "\n",
    "    perplexity = pp ** (1 / (len(tokens) - 3))  # Take the nth root of the product of inverse probabilities\n",
    "    print(perplexity)\n",
    "\n",
    "perplexity_interpolation_for_val()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation for Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0015364723172278\n"
     ]
    }
   ],
   "source": [
    "interpolation_model = InterpolationAddK()\n",
    "\n",
    "def perplexity_interpolation_for_test():\n",
    "    pp = 1\n",
    "    tokenized_validation = text_preprocessor.tokenize_words(text_preprocessor.test_data)\n",
    "    tokens = tokenized_validation['sentences']\n",
    "    for i in range(3, len(tokens)):  # Adjusted loop range for fourgram\n",
    "        current_word = tokens[i]  # Adjusted indexing for fourgram\n",
    "        previous_four_gram = (tokens[i - 3], tokens[i - 2], tokens[i - 1], tokens[i])  # Construct fourgram tuple\n",
    "\n",
    "        probability = interpolation_model.probability(\n",
    "            word=current_word,\n",
    "            given_tri_gram=previous_four_gram[:-1],\n",
    "            \n",
    "        )  # Calculate probability\n",
    "\n",
    "        pp *= (1 / probability) ** (1 / len(tokens))  # Update perplexity\n",
    "\n",
    "    perplexity = pp ** (1 / (len(tokens) - 3))  # Take the nth root of the product of inverse probabilities\n",
    "    print(perplexity)\n",
    "\n",
    "perplexity_interpolation_for_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' <s> computer science is the study of computer science and information technology </s> <s> it is the study of computer science and information technology </s> <s> it is the study of computer science and information technology </s> <s> it is the study of computer science and information technology </s> <s> it is the study of computer science and information technology </s> <s> it is the study of computer science and information technology </s> <s> it is the study of computer science and information technology </s> <s> it is the study of computer science and information technology </s> <s> it is the study of '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = InterpolationAddK(\n",
    "    k=pow(10, -2), \n",
    "    lambda1=pow(10, -5), \n",
    "    lambda2=pow(10, -5),\n",
    "    lambda3=pow(10, -5),\n",
    "    lambda4=pow(10, -1),\n",
    ")\n",
    "\n",
    "def text_generator(tri_gram):\n",
    "\n",
    "    generated_text = ' '\n",
    "    for word in tri_gram:\n",
    "        generated_text = generated_text + word + ' '\n",
    "    for i in range(0, 100):\n",
    "        next_word = model.predict(tri_gram)\n",
    "        tri_gram = tri_gram[1:]\n",
    "        tri_gram = list(tri_gram)\n",
    "        tri_gram.append(next_word)\n",
    "        tri_gram = tuple(tri_gram)\n",
    "        generated_text = generated_text + next_word + ' '\n",
    "    return generated_text\n",
    "    \n",
    "generated = text_generator(('<s>', 'computer', 'science'))\n",
    "generated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
