{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize, ngrams\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.probability import FreqDist\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FourGramProbabilityEstimator:\n",
    "    def __init__(self):\n",
    "        self.unigram_counts = {}\n",
    "        self.bigram_counts = {}\n",
    "        self.trigram_counts = {}\n",
    "        self.fourgram_counts = {}\n",
    "        \n",
    "    def train(self, corpus):\n",
    "        unigram_counts = FreqDist()\n",
    "        bigram_counts = FreqDist()\n",
    "        trigram_counts = FreqDist()\n",
    "        fourgram_counts = FreqDist()\n",
    "\n",
    "        for sentence in corpus['uni_grams']:\n",
    "            for word in sentence:\n",
    "                unigram_counts[word] += 1\n",
    "        \n",
    "\n",
    "        for sentence in corpus['bi_grams']:\n",
    "            for bigram in sentence:\n",
    "                bigram_counts[bigram] += 1\n",
    "\n",
    "        for sentence in corpus['tri_grams']:\n",
    "            for trigram in sentence:\n",
    "                trigram_counts[trigram] += 1\n",
    "\n",
    "        for sentence in corpus['four_grams']:\n",
    "            for fourgram in sentence:\n",
    "                fourgram_counts[fourgram] += 1\n",
    "\n",
    "    def probability_uni(self, word):\n",
    "        return self.unigram_counts.get(word, 0) / sum(self.unigram_counts.values())\n",
    "\n",
    "    def probability_bi(self, prev_word, word):\n",
    "        prev_word_count = self.unigram_counts.get(prev_word, 0)\n",
    "        if prev_word_count > 0:\n",
    "            bigram_key = (prev_word, word)\n",
    "            return self.bigram_counts.get(bigram_key, 0) / prev_word_count\n",
    "        else:\n",
    "            return self.probability_uni(word)\n",
    "\n",
    "    def probability_tri(self, prev_word2, prev_word1, word):\n",
    "        prev_bigram_count = self.bigram_counts.get((prev_word2, prev_word1), 0)\n",
    "        if prev_bigram_count > 0:\n",
    "            trigram_key = (prev_word2, prev_word1, word)\n",
    "            return self.trigram_counts.get(trigram_key, 0) / prev_bigram_count\n",
    "        else:\n",
    "            return self.probability_bi(prev_word1, word)\n",
    "\n",
    "    def probability_four(self, prev_word3, prev_word2, prev_word1, word):\n",
    "        prev_trigram_count = self.trigram_counts.get((prev_word3, prev_word2, prev_word1), 0)\n",
    "        if prev_trigram_count > 0:\n",
    "            fourgram_key = (prev_word3, prev_word2, prev_word1, word)\n",
    "            return self.fourgram_counts.get(fourgram_key, 0) / prev_trigram_count\n",
    "        else:\n",
    "            return self.probability_tri(prev_word2, prev_word1, word)\n",
    "\n",
    "    def probability_backoff(self, prev_word3, prev_word2, prev_word1, word):\n",
    "        fourgram = (prev_word3, prev_word2, prev_word1, word)\n",
    "        trigram = (prev_word2, prev_word1, word)\n",
    "        bigram = (prev_word1, word)\n",
    "\n",
    "        if self.fourgram_counts.get(fourgram, 0) > 0:\n",
    "            return self.probability_four(prev_word3, prev_word2, prev_word1, word)\n",
    "        elif self.trigram_counts.get(trigram, 0) > 0:\n",
    "            return self.probability_tri(prev_word2, prev_word1, word)\n",
    "        elif self.bigram_counts.get(bigram, 0) > 0:\n",
    "            return self.probability_bi(prev_word1, word)\n",
    "        else:\n",
    "            return self.probability_uni(word)\n",
    "\n",
    "    def predict_next_word(self, prev_word3, prev_word2, prev_word1):\n",
    "        # Given the context of the previous three words, predict the next word\n",
    "        possible_next_words = set(self.unigram_counts.keys())  # Consider all possible words\n",
    "        probabilities = []\n",
    "\n",
    "        for word in possible_next_words:\n",
    "            prob = self.probability_backoff(prev_word3, prev_word2, prev_word1, word)\n",
    "            probabilities.append((word, prob))\n",
    "\n",
    "        # Sort the predictions by probability in descending order\n",
    "        predictions = sorted(probabilities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return predictions\n",
    "    \n",
    "    def continue_predicting(self, prev_word3, prev_word2, prev_word1, num_predictions=5):\n",
    "        # Continue predicting the next words based on the previous context\n",
    "        predictions = []\n",
    "\n",
    "        for _ in range(num_predictions):\n",
    "            predicted_words = self.predict_next_word(prev_word3, prev_word2, prev_word1)\n",
    "            if predicted_words:\n",
    "                next_word = predicted_words[0][0]  # Take the most probable next word\n",
    "                predictions.append(next_word)\n",
    "                prev_word3, prev_word2, prev_word1 = prev_word2, prev_word1, next_word\n",
    "\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Predict the next word given the context\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mfourgram_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobability_backoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_word3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_word2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_word1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions for the next word:\u001b[39m\u001b[38;5;124m\"\u001b[39m, predictions)\n",
      "Cell \u001b[0;32mIn[25], line 69\u001b[0m, in \u001b[0;36mFourGramProbabilityEstimator.probability_backoff\u001b[0;34m(self, prev_word3, prev_word2, prev_word1, word)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobability_bi(prev_word1, word)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobability_uni\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 31\u001b[0m, in \u001b[0;36mFourGramProbabilityEstimator.probability_uni\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprobability_uni\u001b[39m(\u001b[38;5;28mself\u001b[39m, word):\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munigram_counts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munigram_counts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "fourgram_estimator = FourGramProbabilityEstimator()\n",
    "fourgram_estimator.train(data)\n",
    "\n",
    "prev_word3 = \"Computer\"\n",
    "prev_word2 = \"Science\"\n",
    "prev_word1 = \"is\"\n",
    "word = \"a\"\n",
    "\n",
    "# Predict the next word given the context\n",
    "predictions = fourgram_estimator.probability_backoff(prev_word3, prev_word2, prev_word1, word)\n",
    "print(\"Predictions for the next word:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity_back_off():\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
