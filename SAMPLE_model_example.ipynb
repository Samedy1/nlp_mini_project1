{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a sample only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_next_word_probability_model_test(word, previous_tri_gram, trigram_counts, fourgram_counts):\n",
    "    # Create the fourgram as a tuple\n",
    "    fourgram_list = list(previous_tri_gram)\n",
    "    fourgram_list.extend(word)\n",
    "    fourgram = tuple(fourgram_list)\n",
    "\n",
    "    # Get the count of the previous word unigram and the bigram from their respective count dictionaries\n",
    "    previous_word_count = trigram_counts.get(previous_tri_gram, 0)\n",
    "    fourgram_count = fourgram_counts.get(fourgram, 0)\n",
    "    \n",
    "    # print(fourgram)\n",
    "\n",
    "    # If the previous word never occurs, return 0 probability\n",
    "    if previous_word_count == 0: return 0 # test\n",
    "\n",
    "    # Calculate the probability \n",
    "    probability = fourgram_count / previous_word_count\n",
    "\n",
    "    if (probability == 0): return 0.00000000000009 # just test, this is not correct\n",
    "\n",
    "    return probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_model_test(previous_tri_gram, unigram_counts, trigram_counts, fourgram_counts, top_n=5):\n",
    "    # Create a list to store predicted words and their probabilities\n",
    "    predictions = []\n",
    "\n",
    "    # Iterate through all possible unigrams\n",
    "    for word in unigram_counts.keys():\n",
    "        # Calculate the probability of the word being the next word\n",
    "        probability = estimate_next_word_probability_model_test(word, previous_tri_gram, trigram_counts, fourgram_counts)\n",
    "\n",
    "        # Add the word and its probability to the list\n",
    "        predictions.append((word, probability))\n",
    "\n",
    "    # Sort the predictions by probability in descending order\n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Return the top n predicted words and their probabilities\n",
    "    return predictions[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "word = ('considered',)\n",
    "previous_tri_gram = ('though', 'more', 'often')\n",
    "trigram_counts = freq_tri\n",
    "fourgram_counts = freq_four\n",
    "\n",
    "probability = estimate_next_word_probability_model_test(word, previous_tri_gram, trigram_counts, fourgram_counts)\n",
    "\n",
    "print(probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def perplexity_model_test():\n",
    "    pp = 1\n",
    "    tokenized_validation = tokenize_words(validation_data)\n",
    "    tokens = tokenized_validation['sentences']\n",
    "    for i in range(0, len(tokens) - 3):\n",
    "        current_word = tokens[i + 3]\n",
    "        previous_tri_grams = (tokens[i], tokens[i + 1], tokens[i + 2])\n",
    "        probability = estimate_next_word_probability_model_test(current_word, previous_tri_gram, freq_tri, freq_four)\n",
    "        pp = pp * ((1 / probability) ** (1 / len(tokens)))\n",
    "        i = i + 1\n",
    "    print(pp)\n",
    "\n",
    "perplexity_model_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_generator(tri_gram):\n",
    "    tmp = tri_gram\n",
    "    #Computer science is\n",
    "    generated_text = ''\n",
    "    for word in tmp:\n",
    "        generated_text = generated_text + word + ' '\n",
    "    for i in range(0, 100):\n",
    "        next_word = language_model_test(tmp, freq_uni, freq_tri, freq_four, top_n=1)\n",
    "        next_word = next_word[0][0][0]\n",
    "        tmp = tmp[1:]\n",
    "        tmp = list(tmp)\n",
    "        tmp.append(next_word)\n",
    "        tmp = tuple(tmp)\n",
    "        generated_text = generated_text + next_word + ' '\n",
    "        # print(next_word, end = ' ')\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Around the <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated = text_generator(('<s>', 'Around', 'the'))\n",
    "generated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
