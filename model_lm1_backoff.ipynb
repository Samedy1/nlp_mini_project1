{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the preprocessed data from the preprocess file\n",
    "from preprocess import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_n_gram_occurance(unique_n_grams, flat_n_grams, unique_word): \n",
    "    word_count_i = 0\n",
    "    for i in unique_n_grams: \n",
    "        if unique_word == i: \n",
    "            for j in flat_n_grams: \n",
    "                if i == j: \n",
    "                   word_count_i +=1\n",
    "            return word_count_i\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_word_count(unique_word): \n",
    "    uni_grams = tokenized_sent['uni_grams']\n",
    "    \n",
    "    unique_word_tuple = (unique_word,)\n",
    "    # Flatten the list of lists\n",
    "    flat_uni_grams = [word for sublist in uni_grams for word in sublist]\n",
    "    \n",
    "    # Convert the flattened list to a set to remove duplicates\n",
    "    unique_words_set_uni_grams = set(flat_uni_grams)\n",
    "    \n",
    "    word_count_i = 0\n",
    "    for i in unique_words_set_uni_grams: \n",
    "        if unique_word_tuple == i: \n",
    "            for j in flat_uni_grams: \n",
    "                if i == j: \n",
    "                   word_count_i +=1\n",
    "            return word_count_i\n",
    "    \n",
    "    return 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backoff\n",
    "# this function utilizes the 3 functions following it\n",
    "def probability_backoff():\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_tri():\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_bi(word1, word2):\n",
    "    bi_grams = tokenized_sent['bi_grams']\n",
    "    \n",
    "    unique_word_tuple = (word1, word2)\n",
    "    \n",
    "    flat_bi_grams = [word for sublist in bi_gram for word in sublist]\n",
    "    \n",
    "    # num_flat_bi_grams = len(flat_bi_grams)\n",
    "    \n",
    "    unique_words_set_bi_grams = set(flat_bi_grams)\n",
    "    \n",
    "    # num_unique_words_set_bi_grams = len(unique_words_set_bi_grams)\n",
    "    \n",
    "    unique_bi_grams_count = unique_n_gram_occurance(unique_words_set_bi_grams, flat_bi_grams, unique_word_tuple)\n",
    "    uni_gram_word = unique_word_tuple[0]\n",
    "    uni_gram_word_count = unigram_word_count(uni_gram_word)\n",
    "    bi_gram_probability = unique_bi_grams_count/uni_gram_word_count\n",
    "    \n",
    "    \n",
    "    return bi_gram_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_uni(unique_word):\n",
    "    uni_grams = tokenized_sent['uni_grams']\n",
    "    \n",
    "    unique_word_tuple = (unique_word,)\n",
    "    # Flatten the list of lists\n",
    "    flat_uni_grams = [word for sublist in uni_grams for word in sublist]\n",
    "    \n",
    "    num_flat_uni_grams = len(flat_uni_grams)\n",
    "    \n",
    "    # Convert the flattened list to a set to remove duplicates\n",
    "    unique_words_set_uni_grams = set(flat_uni_grams)\n",
    "    \n",
    "    # Count the number of unique words\n",
    "    num_unique_words = len(unique_words_set_uni_grams)\n",
    "    \n",
    "    #Create dictionary that store each unique word occurance\n",
    "    unique_uni_grams_count = unique_n_gram_occurance(unique_words_set_uni_grams, flat_uni_grams, unique_word_tuple)\n",
    "    uni_gram_probability = unique_uni_grams_count/num_flat_uni_grams\n",
    "\n",
    "    \n",
    "    return uni_gram_probability\n",
    "     \n",
    "    # return unique_word_prob_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00012295585884667404"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_word_prob = probability_uni('manually')\n",
    "testing_word_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.125"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_prop = probability_bi('computation', 'information')\n",
    "bi_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a model using backoff\n",
    "def language_model_backoff():\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity_back_off():\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "da87a88fed6e90d143d9d0b1bb3133bd9148111863282b0e389216736ed83b9f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
