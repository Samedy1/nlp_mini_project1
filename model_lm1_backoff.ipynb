{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the preprocessed data from the preprocess file\n",
    "from preprocess import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FourGramWithBackOff:\n",
    "    def __init__(self):\n",
    "        self.unigram_freq = {}\n",
    "        self.bigram_freq = {}\n",
    "        self.trigram_freq = {}\n",
    "        self.fourgram_freq = {}\n",
    "        \n",
    "    def train(self):\n",
    "        unigram_freq = FreqDist()\n",
    "        bigram_freq = FreqDist()\n",
    "        trigram_freq = FreqDist()\n",
    "        fourgram_freq = FreqDist()\n",
    "\n",
    "        for sentence in tokenized_sent['uni_grams']:\n",
    "            for word in sentence:\n",
    "                unigram_freq[word] += 1\n",
    "        \n",
    "\n",
    "        for sentence in tokenized_sent['bi_grams']:\n",
    "            for bigram in sentence:\n",
    "                bigram_freq[bigram] += 1\n",
    "\n",
    "        for sentence in tokenized_sent['tri_grams']:\n",
    "            for trigram in sentence:\n",
    "                trigram_freq[trigram] += 1\n",
    "\n",
    "        for sentence in tokenized_sent['four_grams']:\n",
    "            for fourgram in sentence:\n",
    "                fourgram_freq[fourgram] += 1\n",
    "                \n",
    "    def unique_n_gram_occurance(self, unique_n_grams, flat_n_grams, unique_word): \n",
    "        word_count_i = 0\n",
    "        for i in unique_n_grams: \n",
    "            if unique_word == i: \n",
    "                for j in flat_n_grams: \n",
    "                    if i == j: \n",
    "                        word_count_i +=1\n",
    "                return word_count_i\n",
    "        return 0\n",
    "    \n",
    "    def unigram_word_count(self, unique_word): \n",
    "        uni_grams = tokenized_sent['uni_grams']\n",
    "        \n",
    "        unique_word_tuple = (unique_word,)\n",
    "        # Flatten the list of lists\n",
    "        flat_uni_grams = [word for sublist in uni_grams for word in sublist]\n",
    "        \n",
    "        # Convert the flattened list to a set to remove duplicates\n",
    "        unique_words_set_uni_grams = set(flat_uni_grams)\n",
    "        \n",
    "        word_count_i = 0\n",
    "        for i in unique_words_set_uni_grams: \n",
    "            if unique_word_tuple == i: \n",
    "                for j in flat_uni_grams: \n",
    "                    if i == j: \n",
    "                        word_count_i +=1\n",
    "                return word_count_i\n",
    "        \n",
    "        return 0 \n",
    "\n",
    "    def probability_uni(self, unique_word):\n",
    "        uni_grams = tokenized_sent['uni_grams']\n",
    "        \n",
    "        unique_word_tuple = (unique_word,)\n",
    "        # Flatten the list of lists\n",
    "        flat_uni_grams = [word for sublist in uni_grams for word in sublist]\n",
    "        \n",
    "        num_flat_uni_grams = len(flat_uni_grams)\n",
    "        \n",
    "        # Convert the flattened list to a set to remove duplicates\n",
    "        unique_words_set_uni_grams = set(flat_uni_grams)\n",
    "        \n",
    "        # Count the number of unique words\n",
    "        num_unique_words = len(unique_words_set_uni_grams)\n",
    "        \n",
    "        #Create dictionary that store each unique word occurance\n",
    "        unique_uni_grams_count = self.unique_n_gram_occurance(unique_words_set_uni_grams, flat_uni_grams, unique_word_tuple)\n",
    "        uni_gram_probability = unique_uni_grams_count/num_flat_uni_grams\n",
    "\n",
    "        \n",
    "        return uni_gram_probability\n",
    "    \n",
    "    def probability_bi(self, word1, word2):\n",
    "        bi_grams = tokenized_sent['bi_grams']\n",
    "        uni_grams = tokenized_sent['uni_grams']\n",
    "        \n",
    "        unique_word_tuple = (word1, word2)\n",
    "        \n",
    "        flat_uni_grams = [word for sublist in uni_grams for word in sublist]\n",
    "        flat_bi_grams = [word for sublist in bi_grams for word in sublist]\n",
    "        \n",
    "        # num_flat_bi_grams = len(flat_bi_grams)\n",
    "        \n",
    "        unique_words_set_uni_grams = set(flat_uni_grams)\n",
    "        unique_words_set_bi_grams = set(flat_bi_grams)\n",
    "        \n",
    "        # num_unique_words_set_bi_grams = len(unique_words_set_bi_grams)\n",
    "        \n",
    "        unique_bi_grams_count = self.unique_n_gram_occurance(unique_words_set_bi_grams, flat_bi_grams, unique_word_tuple)\n",
    "        uni_gram_word = (unique_word_tuple[0],)\n",
    "        uni_gram_word_count = self.unique_n_gram_occurance(unique_words_set_uni_grams, flat_uni_grams, uni_gram_word)\n",
    "        if uni_gram_word_count != 0: \n",
    "            bi_gram_probability = unique_bi_grams_count/uni_gram_word_count\n",
    "        else: \n",
    "            bi_gram_probability = float('inf')  # Set to infinity\n",
    "        \n",
    "        \n",
    "        return bi_gram_probability\n",
    "    \n",
    "    def probability_tri(self, word1, word2, word3):\n",
    "        tri_grams = tokenized_sent['tri_grams']\n",
    "        bi_grams = tokenized_sent['bi_grams']\n",
    "        uni_grams = tokenized_sent['uni_grams']\n",
    "        \n",
    "        unique_word_tuple = (word1, word2, word3)\n",
    "        \n",
    "        flat_uni_grams = [word for sublist in uni_grams for word in sublist]\n",
    "        flat_bi_grams = [word for sublist in bi_grams for word in sublist]\n",
    "        flat_tri_grams = [word for sublist in tri_grams for word in sublist]\n",
    "        \n",
    "        unique_words_set_uni_grams = set(flat_uni_grams)\n",
    "        unique_words_set_bi_grams = set(flat_bi_grams)\n",
    "        unique_words_set_tri_grams = set(flat_tri_grams)\n",
    "        \n",
    "        unique_tri_grams_count = self.unique_n_gram_occurance(unique_words_set_tri_grams, flat_tri_grams, unique_word_tuple)\n",
    "        bi_gram_word = (unique_word_tuple[0], unique_word_tuple[1])\n",
    "        bi_gram_word_count = self.unique_n_gram_occurance(unique_words_set_bi_grams, flat_bi_grams, bi_gram_word)\n",
    "        if bi_gram_word_count != 0:\n",
    "            tri_gram_probability = unique_tri_grams_count / bi_gram_word_count\n",
    "        else:\n",
    "            tri_gram_probability = float('inf')  # Set to infinity\n",
    "        \n",
    "        return tri_gram_probability\n",
    "\n",
    "    def probability_four(self, word1, word2, word3, word4):\n",
    "        four_grams = tokenized_sent['four_grams']\n",
    "        tri_grams = tokenized_sent['tri_grams']\n",
    "        bi_grams = tokenized_sent['bi_grams']\n",
    "        uni_grams = tokenized_sent['uni_grams']\n",
    "        \n",
    "        unique_word_tuple = (word1, word2, word3, word4)\n",
    "        \n",
    "        flat_uni_grams = [word for sublist in uni_grams for word in sublist]\n",
    "        flat_bi_grams = [word for sublist in bi_grams for word in sublist]\n",
    "        flat_tri_grams = [word for sublist in tri_grams for word in sublist]\n",
    "        flat_four_grams = [word for sublist in four_grams for word in sublist]\n",
    "        \n",
    "        unique_words_set_uni_grams = set(flat_uni_grams)\n",
    "        unique_words_set_bi_grams = set(flat_bi_grams)\n",
    "        unique_words_set_tri_grams = set(flat_tri_grams)\n",
    "        unique_words_set_four_grams = set(flat_four_grams)\n",
    "        \n",
    "        unique_four_grams_count = self.unique_n_gram_occurance(unique_words_set_four_grams, flat_four_grams, unique_word_tuple)\n",
    "        tri_gram_word = (unique_word_tuple[0], unique_word_tuple[1], unique_word_tuple[2])\n",
    "        tri_gram_word_count = self.unique_n_gram_occurance(unique_words_set_tri_grams, flat_tri_grams, tri_gram_word)\n",
    "        if tri_gram_word_count != 0:\n",
    "            four_gram_probability = unique_four_grams_count / tri_gram_word_count\n",
    "        else:\n",
    "            four_gram_probability = float('inf')  # Set to infinity\n",
    "        \n",
    "        return four_gram_probability\n",
    "\n",
    "    def probability_backoff(self, word1, word2, word3, word4):\n",
    "        fourgram_prob = self.probability_four(word1, word2, word3, word4)\n",
    "        \n",
    "        if fourgram_prob != 0 and fourgram_prob != float('inf'):\n",
    "            return fourgram_prob\n",
    "        else:\n",
    "            trigram_prob = self.probability_tri(word2, word3, word4)\n",
    "            \n",
    "            if trigram_prob != 0 and trigram_prob != float('inf'):\n",
    "                return trigram_prob\n",
    "            else:\n",
    "                bigram_prob = self.probability_bi(word3, word4)\n",
    "                \n",
    "                if bigram_prob != 0 and bigram_prob != float('inf'):\n",
    "                    return bigram_prob\n",
    "                else:\n",
    "                    unigram_prob = self.probability_uni(word4)\n",
    "                    return unigram_prob\n",
    "                \n",
    "    def predict_next_word(self, word1, word2, word3):\n",
    "        uni_grams = tokenized_sent['uni_grams']\n",
    "        possible_next_words = set(word for sublist in uni_grams for word in sublist)\n",
    "        max_probability = 0\n",
    "        predicted_word = \"\"\n",
    "\n",
    "        for next_word in possible_next_words:\n",
    "            fourgram_prob = self.probability_backoff(word1, word2, word3, next_word)\n",
    "            \n",
    "            if fourgram_prob > max_probability:\n",
    "                max_probability = fourgram_prob\n",
    "                predicted_word = next_word\n",
    "\n",
    "        return predicted_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FourGramWithBackOff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004057543341940244"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_word_prob = model.probability_uni('computer')\n",
    "testing_word_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11956521739130435"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_prop = model.probability_bi('is', 'a')\n",
    "bi_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tri_prop = model.probability_tri('science', 'is', 'a')\n",
    "tri_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "four_prop = model.probability_four('computer', 'science', 'is', 'the')\n",
    "four_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backoff_prop = model.probability_backoff('computer', 'science', 'is', 'the')\n",
    "backoff_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = model.predict_next_word('computer', 'science', 'a')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity_back_off():\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "da87a88fed6e90d143d9d0b1bb3133bd9148111863282b0e389216736ed83b9f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
