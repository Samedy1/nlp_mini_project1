{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the preprocessed data from the preprocess file\n",
    "from preprocess import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_n_gram_occurance(unique_n_grams, flat_n_grams, unique_word): \n",
    "    word_count_i = 0\n",
    "    for i in unique_n_grams: \n",
    "        if unique_word == i: \n",
    "            for j in flat_n_grams: \n",
    "                if i == j: \n",
    "                   word_count_i +=1\n",
    "    return word_count_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_word_count(unique_word): \n",
    "    uni_grams = tokenized_sent['uni_grams']\n",
    "    \n",
    "    unique_word_tuple = (unique_word,)\n",
    "    # Flatten the list of lists\n",
    "    flat_uni_grams = [word for sublist in uni_grams for word in sublist]\n",
    "    \n",
    "    # Convert the flattened list to a set to remove duplicates\n",
    "    unique_words_set_uni_grams = set(flat_uni_grams)\n",
    "    \n",
    "    word_count_i = 0\n",
    "    for i in unique_words_set_uni_grams: \n",
    "        if unique_word_tuple == i: \n",
    "            for j in flat_uni_grams: \n",
    "                if i == j: \n",
    "                   word_count_i +=1\n",
    "            return word_count_i\n",
    "    \n",
    "    return 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backoff\n",
    "# this function utilizes the 3 functions following it\n",
    "def probability_backoff():\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_tri():\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_bi(word1, word2):\n",
    "    bi_grams = tokenized_sent['bi_grams']\n",
    "    uni_grams = tokenized_sent['uni_grams']\n",
    "    \n",
    "    unique_word_tuple = (word1, word2)\n",
    "    \n",
    "    flat_uni_grams = [word for sublist in uni_grams for word in sublist]\n",
    "    flat_bi_grams = [word for sublist in bi_grams for word in sublist]\n",
    "    \n",
    "    # num_flat_bi_grams = len(flat_bi_grams)\n",
    "    \n",
    "    unique_words_set_uni_grams = set(flat_uni_grams)\n",
    "    unique_words_set_bi_grams = set(flat_bi_grams)\n",
    "    \n",
    "    # num_unique_words_set_bi_grams = len(unique_words_set_bi_grams)\n",
    "    \n",
    "    unique_bi_grams_count = unique_n_gram_occurance(unique_words_set_bi_grams, flat_bi_grams, unique_word_tuple)\n",
    "    uni_gram_word = (unique_word_tuple[0],)\n",
    "    uni_gram_word_count = unique_n_gram_occurance(unique_words_set_uni_grams, flat_uni_grams, uni_gram_word)\n",
    "    if uni_gram_word_count != 0: \n",
    "        bi_gram_probability = unique_bi_grams_count/uni_gram_word_count\n",
    "    else: \n",
    "        bi_gram_probability = float('inf')  # Set to infinity\n",
    "    \n",
    "    \n",
    "    return bi_gram_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_uni(unique_word):\n",
    "    uni_grams = tokenized_sent['uni_grams']\n",
    "    \n",
    "    unique_word_tuple = (unique_word,)\n",
    "    # Flatten the list of lists\n",
    "    flat_uni_grams = [word for sublist in uni_grams for word in sublist]\n",
    "    \n",
    "    num_flat_uni_grams = len(flat_uni_grams)\n",
    "    \n",
    "    # Convert the flattened list to a set to remove duplicates\n",
    "    unique_words_set_uni_grams = set(flat_uni_grams)\n",
    "    \n",
    "    # Count the number of unique words\n",
    "    num_unique_words = len(unique_words_set_uni_grams)\n",
    "    \n",
    "    #Create dictionary that store each unique word occurance\n",
    "    unique_uni_grams_count = unique_n_gram_occurance(unique_words_set_uni_grams, flat_uni_grams, unique_word_tuple)\n",
    "    uni_gram_probability = unique_uni_grams_count/num_flat_uni_grams\n",
    "\n",
    "    \n",
    "    return uni_gram_probability\n",
    "     \n",
    "    # return unique_word_prob_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0025820730357801547"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_word_prob = probability_uni('information')\n",
    "testing_word_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Value Count: 1\n",
      "('specifically', 'for')\n",
      "('specifically',)\n",
      "Unigram Value Count: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_prop = probability_bi('specifically', 'for')\n",
    "bi_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_prop_1 = probability_bi('warehouses', 'using')\n",
    "bi_prop_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a model using backoff\n",
    "def language_model_backoff():\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity_back_off():\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "da87a88fed6e90d143d9d0b1bb3133bd9148111863282b0e389216736ed83b9f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
